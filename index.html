<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>From Dataset to Forest Plot</title>


    <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>


    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src='https://kit.fontawesome.com/a076d05399.js' crossorigin='anonymous'></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>

<!-- Title and TL;DR -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h3 class="title is-1 publication-title">Automating Biomedical Evidence Synthesis: From Retrieval to Forest Plots</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Massimiliano Pronesti, et al.</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#BibTeX" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-quote-right"></i>
                  </span>
                  <span>BibTeX</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR Section -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <blockquote class="blockquote-tldr">
        <div class="tldr-header">TL;DR</div>
        <ul>
          <li>A three-paper journey from evidence extraction to end-to-end forest plot generation.</li>
          <li>Each paper builds on the previous: from data and retrieval, to numeric reasoning, to automation.</li>
          <li>Final goal: enabling scalable, interpretable synthesis for biomedical meta-analyses.</li>
        </ul>
      </blockquote>
    </div>
  </div>
</section>


  <!-- Paper 1 -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies</h2>

          <!-- Authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://research.ibm.com/people/massimiliano-pronesti">Massimiliano Pronesti</a><sup>1,2</sup>,
            </span>

            <span>
              <a href="https://research.ibm.com/people/joao-bettencourt-silva">Joao Bettencourt-Silva</a><sup>1</sup>,
            </span>

            <span>
              <a href="">Paul Flanagan</a><sup>2</sup>,
            </span>

            <span>
              <a href="https://research.ibm.com/people/alessandra-pascale">Alessandra Pascale</a><sup>1</sup>,
            </span>

            <span>
              <a href="">Oisín Redmond</a><sup>2</sup>,
            </span>

            <span>
              <a href="https://www.dcu.ie/computing/people/anya-belz">Anya Belz</a><sup>2</sup>,
            </span>

            <span>
              <a href="https://yufanghou.github.io/">Yufang Hou</a><sup>1,3</sup>
            </span>


            </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>IBM Research Europe - Ireland, <sup>2</sup>Dublin City University <br> <sup>3</sup>IT:U University Transformation Austria</span>
          </div>

          <div class="publication-links">
            <a href="https://arxiv.org/pdf/2505.06186" class="external-link button is-normal is-rounded is-dark" target="_blank">
              <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
            </a>
            <a href="https://arxiv.org/abs/2505.06186" class="external-link button is-normal is-rounded is-dark" target="_blank">
              <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
            </a>
          </div>
          <img src="static/images/paper1_fig1.png" alt="paper1_figure1" style="margin-top: 20px; width: 45vw;" />

          <br><br>

          <h3 class="title is-3">Abstract</h3>
          <div class="content has-text-justified">
            <p>Extracting scientific evidence from biomedical studies for clinical research questions (e.g., <i>Does stem cell transplantation improve quality of life in patients with medically refractory Crohn's disease compared to placebo?</i>) is a crucial step in synthesising biomedical evidence. </p>
            <p>In this paper, we focus on the task of document-level scientific evidence extraction for clinical questions with conflicting evidence. To support this task, we create a dataset called CochraneForest, leveraging forest plots from Cochrane systematic reviews. It comprises 202 annotated forest plots, associated clinical research questions, full texts of studies, and study-specific conclusions. Building on CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a retrieval-augmented generation framework designed to tackle the unique challenges of evidence extraction. Our experiments show that URCA outperforms the best existing methods by up to 10.3% in F1 score on this task. However, the results also underscore the complexity of CochraneForest, establishing it as a challenging testbed for advancing automated evidence synthesis systems.</p>
          </div>

          <h3 class="title is-3">The <span id="small-caps">CochraneForest</span> Dataset</h3>
          <div class="content has-text-justified">
            The <span id="small-caps">CochraneForest</span> dataset comprises 202 forest plots extracted from 48 Cochrane systematic reviews, covering 263 unique studies and 923 research question-study pairs. Each plot is annotated with a clinical research question, study-level conclusions, and full-text papers associated with each study. To build the dataset, reviews were filtered to include only those with fully accessible study texts and at least two studies with conflicting conclusions. Research questions were generated and refined using LLMs, and study conclusions were labeled based on forest plot confidence intervals. Three annotation tasks ensured consistency across research questions, intervention labels, and conclusions. Inter-annotator agreement showed high semantic consistency, establishing CochraneForest as a reliable and challenging benchmark for evidence synthesis.
          </div>

          <h3 class="title is-3">Method</h3>
          <img src="static/images/urca.png" alt="urca" style="margin-top: 20px;" />
          <div class="content has-text-justified">
            <p>Our method is divided into three phases:</p>
            <ol>
              <li>
                <strong>Uniform Retrieval</strong><br>
                Given a clinical question and the associated set of study papers, we retrieve a fixed number of passages from each source to ensure balanced evidence coverage. This prevents over-representation of longer studies and improves the diversity of retrieved content.
              </li>

              <li>
                <strong>Clustering and Knowledge Extraction</strong><br>
                Retrieved passages are embedded, reduced in dimensionality with UMAP, and grouped using Gaussian Mixture Models. For each cluster, a large language model (LLM) extracts query-relevant evidence, discarding unrelated content and highlighting meaningful insights.
              </li>

              <li>
                <strong>Answer Generation</strong><br>
                The extracted evidence passages are fed to the LLM alongside the clinical question to generate the final answer. This stage synthesizes semantically aligned, cluster-aware information into a concise conclusion for each study.
              </li>
            </ol>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper 2 -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning</h2>

          <!-- Authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://research.ibm.com/people/massimiliano-pronesti">Massimiliano Pronesti</a><sup>1,2</sup>,
            </span>

            <span class="author-block">
              <a href="">Michela Lorandi</a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="">Paul Flanagan</a><sup>2</sup>,
            </span>

            <span class="author-block">
              <a href="">Oisín Redmond</a><sup>2</sup>,
            </span>

            <span class="author-block">
              <a href="https://www.dcu.ie/computing/people/anya-belz">Anya Belz</a><sup>2</sup>,
            </span>

            <span class="author-block">
              <a href="https://yufanghou.github.io/">Yufang Hou</a><sup>2</sup>
            </span>


            </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>IBM Research Europe - Ireland, <sup>2</sup>Dublin City University <br> <sup>3</sup>IT:U University Transformation Austria</span>
          </div>

          <div class="publication-links">
            <a href="https://arxiv.org/pdf/2505.22928" class="external-link button is-normal is-rounded is-dark" target="_blank">
              <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
            </a>
            <a href="https://arxiv.org/abs/2505.22928" class="external-link button is-normal is-rounded is-dark" target="_blank">
              <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
            </a>
          </div>
          <img src="static/images/paper2_fig1.png" alt="paper2_figure1" style="margin-top: 20px; width: 65%;" />

          <h3 class="title is-3">Abstract</h3>
          <div class="content has-text-justified">
           <p>
             Systematic reviews in medicine play a critical role in evidence-based decision-making by aggregating findings from multiple studies. A central bottleneck in automating this process is extracting numeric evidence and determining study-level conclusions for specific outcomes and comparisons. Prior work has framed this problem as a textual inference task by retrieving relevant content fragments and inferring conclusions from them. However, such approaches often rely on shallow textual cues and fail to capture the underlying numeric reasoning behind expert assessments.
           </p>
            <p>
              In this work, we conceptualise the problem as one of quantitative reasoning. Rather than inferring conclusions from surface text, we extract structured numerical evidence (e.g., event counts or standard deviations) and apply domain knowledge informed logic to derive outcome-specific conclusions. We develop a numeric reasoning system composed of a numeric data extraction model and an effect estimate component, enabling more accurate and interpretable inference aligned with the domain expert principles. We train the numeric data extraction model using different strategies, including supervised fine-tuning (SFT) and reinforcement learning (RL) with a new value reward model.
            </p>
            <p>
              When evaluated on the CochraneForest benchmark, our best-performing approach &mdash; using RL to train a small-scale number extraction model &mdash; yields up to a 21% absolute improvement in F1 score over retrieval-based systems and outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our results demonstrate the promise of reasoning-driven approaches for automating systematic evidence synthesis.
            </p>
         </div>

<h3 class="title is-3">Method</h3>
<div class="content has-text-justified">
  <p>Our method is divided into two stages:</p>
  <ol>
    <li>
      <strong>Numeric Evidence Extraction:</strong> A compact language model is trained to extract structured numerical data (e.g., means, standard deviations, event counts) from full-text clinical trial papers. We explore three training strategies:
      <ul>
        <li>Supervised Fine-Tuning (SFT)</li>
        <li>SFT with reasoning traces augmentation</li>
        <li>Reinforcement Learning (RL) with <a href="https://arxiv.org/pdf/2402.03300"><b>Group Relative Policy Optimization</b></a> and custom rule-based rewards</li>
      </ul>
    </li>
    <li>
      <strong>Effect Estimation and Inference:</strong> Using domain-specific statistical formulas, we compute effect sizes (mean differences or risk ratios) and corresponding 95% confidence intervals. Study-level conclusions are then derived by checking whether the confidence interval supports a statistically significant effect.
    </li>
  </ol>
  <p>This structured approach allows us to directly generate entries for forest plots, enabling more transparent and automatable evidence synthesis.</p>
</div>

<h3 class="title is-3">Key Results</h3>
<div class="content has-text-justified">
  <p>
    The RL-trained numeric reasoning model achieves substantial improvements:
  </p>
  <ol>
    <li>Up to <strong>21 F1 points</strong> higher than retrieval-based baselines on <span id="small-caps">CochraneForest</span>.</li>
    <li>Outperforms GPT-4 and other large models by up to <strong>9 F1 points</strong>, despite being significantly smaller (7B vs 400B+).</li>
    <li>Produces better-structured, traceable, and factually grounded reasoning with fewer hallucinations.</li>
    <li>Develops numerical reasoning capabilities beyond the mere extraction.</li>
  </ol>
</div>
  <img src="static/images/paper2_fig2.png" alt="Model performance vs retrieval precision" style="max-width:70%; height:auto; margin-bottom:1em;">


        </div>

      </div>
    </div>
  </section>

  <!-- Paper 3 -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"><span id="small-caps">AutoForest</span>: Automatically Generating Forest Plots from Biomedical Studies</h2>

          <!-- Authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://research.ibm.com/people/massimiliano-pronesti">Massimiliano Pronesti</a><sup>1,2</sup>,
            </span>

            <span>
              <a href="">Paul Flanagan</a><sup>2</sup>,
            </span>

            <span>
              <a href="">Oisín Redmond</a><sup>2</sup>,
            </span>

            <span>
              <a href="https://research.ibm.com/people/joao-bettencourt-silva">Joao Bettencourt-Silva</a><sup>1</sup>,
            </span>

            <span>
              <a href="https://www.nds.ox.ac.uk/team/gurdeep-mannu">Gurdeep S. Mannu</a><sup>3</sup>,
            </span>

            <span>
              <a href="https://www.dcu.ie/computing/people/anya-belz">Anya Belz</a><sup>2</sup>,
            </span>

            <span>
              <a href="https://yufanghou.github.io/">Yufang Hou</a><sup>1,4</sup>
            </span>


            </div>

          <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>IBM Research Europe - Ireland, <sup>2</sup>Dublin City University, <sup>3</sup>University of Oxford</span> <br> <sup>4</sup>IT:U University Transformation Austria</span>
          </div>

            <div class="publication-links">
            <a class="external-link button is-normal is-rounded is-dark" target="_blank">
              <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper - Coming soon</span>
            </a>
            <a class="external-link button is-normal is-rounded is-dark" target="_blank">
              <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv - Coming soon</span>
            </a>
          </div>

          <br>

          <img src="static/images/paper3_fig1.png" alt="paper3_figure1" style="margin-top: 20px; margin-bottom: 20px; margin-left: 100px; width: 75%;" />
          <h3 class="title is-3">Abstract</h3>
          <div class="content has-text-justified">
          <p>
            Systematic reviews rely on forest plots to synthesise quantitative evidence across biomedical studies, but generating them remains a fragmented and labour-intensive process. Researchers must interpret complex clinical texts, manually extract outcome data from trials, define appropriate interventions and comparators, harmonise inconsistent study designs, and carry out meta-analytic computations—typically using specialised software that demands structured inputs and domain expertise. While recent work has demonstrated that large language models can extract study-level data from unstructured text, no existing system automates the complete pipeline from raw documents to synthesised forest plots.
          </p>
           <p>
             To address this gap, we introduce <span id="small-caps">AutoForest</span>, the first end-to-end system that generates publication-ready forest plots directly from biomedical papers. Given one or more study papers, <span id="small-caps">AutoForest</span> automatically suggests ICO (Intervention, Comparator, Outcome) elements, extracts outcome data, performs statistical synthesis, and renders the final forest plot—all with minimal human input. We describe the system architecture, user interface and demonstrate its effectiveness on real-world examples through a user study, showing how <span id="small-caps">AutoForest</span> accelerates evidence synthesis and substantially lowers the barrier to conducting meta-analyses.
           </p>
          </div>

<!--          <h3 class="title is-3">Key Results</h3>-->
<!--          <div class="content has-text-justified">-->
<!--          <ul>-->
<!--            <li>Reduces meta-analysis data synthesis from hours to minutes.</li>-->
<!--            <li>Supports interactive editing, human-in-the-loop validation, and end-to-end plot preview.</li>-->
<!--            <li>paper3_figure1 outlines the full AUTOFOREST workflow.</li>-->
<!--          </ul>-->
<!--        </div>-->

          <div>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/taRehpTgjTU" title="AUTOFOREST demo" frameborder="0" allowfullscreen></iframe>
          </div>
          <div class="notification is-info is-light" style="display: flex; align-items: center;">
  <span class="icon has-text-info" style="margin-right: 0.5em;">
    <i class="fas fa-calendar-alt"></i>
  </span>
  <span>
    Want to see a live demo and you're at ACL? Visit the IBM booth on <strong>Monday from 10:30 am to 11:30 am and from 6:30 pm to 7:30 pm</strong>!
  </span>
</div>
      </div>
    </div>
    </div>
  </section>

  <!-- BibTeX Section -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{pronesti-etal-2025-query,
    title = "Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies",
    author = "Pronesti, Massimiliano  and
      Bettencourt-Silva, Joao H  and
      Flanagan, Paul  and
      Pascale, Alessandra  and
      Redmond, Ois{\'i}n  and
      Belz, Anya  and
      Hou, Yufang",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1359/",
    pages = "28034--28051",
    ISBN = "979-8-89176-251-0",
    abstract = "Extracting scientific evidence from biomedical studies for clinical research questions (e.g., Does stem cell transplantation improve quality of life in patients with medically refractory Crohn{'}s disease compared to placebo?) is a crucial step in synthesising biomedical evidence. In this paper, we focus on the task of document-level scientific evidence extraction for clinical questions with conflicting evidence. To support this task, we create a dataset called CochraneForest leveraging forest plots from Cochrane systematic reviews. It comprises 202 annotated forest plots, associated clinical research questions, full texts of studies, and study-specific conclusions. Building on CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a retrieval-augmented generation framework designed to tackle the unique challenges of evidence extraction. Our experiments show that URCA outperforms the best existing methods by up to 10.3{\%} in F1 score on this task. However, the results also underscore the complexity of CochraneForest, establishing it as a challenging testbed for advancing automated evidence synthesis systems."
}

@article{pronesti2025enhancing,
  title={Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning},
  author={Pronesti, Massimiliano and Lorandi, Michela and Flanagan, Paul and Redmond, Ois\'in and Belz, Anya and Hou, Yufang},
  journal={arXiv preprint arXiv:2505.22928},
  year={2025},
  url={https://arxiv.org/abs/2505.22928}
}

@unpublished{pronesti2025autoforest,
  title={AutoForest: Automatically Generating Forest Plots from Biomedical Studies with End-to-End Evidence Extraction and Synthesis},
  author={Pronesti, Massimiliano and Flanagan, Paul and Redmond, Ois\'in and Bettencourt-Silva, Joao and Mannu, Gurdeep S. and Belz, Anya and Hou, Yufang},
  note={Coming soon},
  year={2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>
